{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNO50TpUP0Afd98yQQfj/hD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adnaen/ai-dev-toolbox/blob/main/python-tools/transformers/day_4_inference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Inference\n",
        "\n",
        "it means interact with the model"
      ],
      "metadata": {
        "id": "lyYBiFsmktG6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference Techniques\n",
        "\n",
        "### **temperature**\n",
        "\n",
        "- It the measure of how much the model need to be creative.\n",
        "- With high temperature the model generate more words its own.\n",
        "- But sometime that can cause hallusination\n",
        "- Usually between 0.1 to 2.0\n",
        "- 1.0 = default, no change\n",
        "- less than 1.0 = fixed or confident answeres.\n",
        "- greater than 1.0 = more creative, but sometime generate answer that not meaningfull.\n",
        "- `best range : 0.6 - 1.0`\n",
        "\n",
        "\n",
        "### **do_sample**\n",
        "\n",
        "- It for let model know pick probability randomly. rather than pick from highest to lowest probability order.\n",
        "- It is boolean\n",
        "\n",
        "### **top_k**\n",
        "\n",
        "- It only show the top_k probability distribution.\n",
        "- If we set to 20, it sort the entire probability into hightest to lowest,  it pick 20 probability values for prediction\n",
        "- It is whole numbers\n",
        "\n",
        "### **top_p**\n",
        "\n",
        "- When it set it, it randomly find cumulative sum of the probability and stop when the sum result match the top_p value, and it use that much value for prediction.\n",
        "- It is value between 0 to 1"
      ],
      "metadata": {
        "id": "YJ0nPMCZ2DgE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference with LLM\n",
        "\n",
        "- we can inference a model using `transformers` with 2 way\n",
        "\n",
        "    1. `pipeline()` :\n",
        "        - there is a high-level api function that automatically load model, tokenizer and decode the output token back.\n",
        "        - `cons` : this not ideal for production, it give less control over model.\n",
        "\n",
        "    2. `Manual Loading`\n",
        "        - Recomented approach for production.\n",
        "        - in this, we manualy need to load model, and its tokenizer. and manualy encode/decode tokens as needed.\n",
        "        - But this can tune the model performance (hyperaparams) as out preference."
      ],
      "metadata": {
        "id": "jfz5Jm7AO073"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "bkHWgYpGX_8-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "torch.cuda.is_available()"
      ],
      "metadata": {
        "id": "NEYG0Zm3mpIA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M-mcDdtjkpxX"
      },
      "outputs": [],
      "source": [
        "model_name: str = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference with Manual"
      ],
      "metadata": {
        "id": "FTLOwEyle1Zx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer"
      ],
      "metadata": {
        "id": "Yh9Rw4DbOq_f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`as we know LLM model return per token in each generatation. some model expect proper structured input from user that contain in model tokenizer_config.json file in HF hug, so when inference with that model we need to set the chat template`"
      ],
      "metadata": {
        "id": "SzXBiK2xBaB5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "message = [\n",
        "    {\n",
        "        \"role\"    : \"system\",\n",
        "        \"content\" : \"you are a helpfull assistant and kindness.\"\n",
        "    },\n",
        "    {\n",
        "        \"role\"    : \"user\",\n",
        "        \"content\" : \"what is the capital of India.\"\n",
        "    }\n",
        "]"
      ],
      "metadata": {
        "id": "4RsVRLf-84Kq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "x2EO0b4iBuLE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "        pretrained_model_name_or_path = model_name,\n",
        "        torch_dtype                   = \"auto\",\n",
        "        device_map                    = \"auto\"\n",
        "    )"
      ],
      "metadata": {
        "id": "VQg4DRkcOw9S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ip_ids = tokenizer.apply_chat_template(message, return_tensors=\"pt\", tokenize=True)\n",
        "ip_ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kuOV5wWq3WeM",
        "outputId": "c1ac273e-80ae-4a2b-c3bd-d4f1a3788332"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[  529, 29989,  5205, 29989, 29958,    13,  6293,   526,   263,  1371,\n",
              "          8159, 20255,   322,  2924,  2264, 29889,     2, 29871,    13, 29966,\n",
              "         29989,  1792, 29989, 29958,    13,  5816,   338,   278,  7483,   310,\n",
              "          7513, 29889,     2, 29871,    13]])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_ids = model.generate(ip_ids, max_new_tokens=20, temperature=0.8)\n",
        "output_ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OvdW3g87Zqbm",
        "outputId": "441e3b10-7dce-4e6c-b8a0-8e090d7ac063"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[  529, 29989,  5205, 29989, 29958,    13,  6293,   526,   263,  1371,\n",
              "          8159, 20255,   322,  2924,  2264, 29889,     2, 29871,    13, 29966,\n",
              "         29989,  1792, 29989, 29958,    13,  5816,   338,   278,  7483,   310,\n",
              "          7513, 29889,     2, 29871,    13, 29966, 29989,   465, 22137, 29989,\n",
              "         29958,    13,  1576,  7483,   310,  7513,   338,  1570,  5556,  2918,\n",
              "         29889,     2]])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(output_ids[0], skip_special_tokens=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "zJbWdTEAJ4iI",
        "outputId": "d4e22cfc-20ae-47cb-969c-839d6948941c"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<|system|>\\nyou are a helpfull assistant and kindness. \\n<|user|>\\nwhat is the capital of India. \\n<|assistant|>\\nThe capital of India is New Delhi.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Streaming Output\n",
        "\n",
        "- like, we see in ChatGPT application the LLM response is generate like a word by word flow that is called 'Streaming Output'\n",
        "- let's look at how we can do the same."
      ],
      "metadata": {
        "id": "OO4nQIXevpUS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TextIteratorStreamer\n",
        "from threading import Thread\n",
        "\n",
        "streamer = TextIteratorStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
        "\n",
        "text = tokenizer.apply_chat_template(message, tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "input_ids = tokenizer(text, return_tensors=\"pt\")\n",
        "\n",
        "model_kwargs = dict(\n",
        "    do_sample=True,\n",
        "    temperature=0.8,\n",
        "    top_p=0.9,\n",
        "    top_k=40,\n",
        "    max_new_tokens=100,\n",
        "    input_ids=input_ids[\"input_ids\"],\n",
        "    streamer=streamer\n",
        ")\n",
        "\n",
        "infr_thread = Thread(\n",
        "    target=model.generate,\n",
        "    kwargs=model_kwargs,\n",
        ")\n",
        "\n",
        "infr_thread.start()\n",
        "\n",
        "for token in streamer:\n",
        "    if token != \"\":\n",
        "        print(token, flush=True, end=\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "USjdUk2G8ygQ",
        "outputId": "6b0caf88-f705-43ca-e992-a9d38072731d"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Yes, the capital of India is New Delhi."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### In My understand\n",
        "\n",
        "- most of the model can load with transformers library need GPU to better inference. with a no GPU system can only find a few model to inference better with transformers.\n",
        "\n",
        "- when inference this lib, using manual model load and inference is better than pipeline() approach,\n",
        "\n",
        "- when we dont have GPU or better computer resource, that were the `llama.cpp` comes into play. we can load LLMs into low end system (cpu only) with llama.cpp. that develop mainly with LlaMA architecure so it mainly compact with llama model.\n",
        "\n",
        "- we can improve the model output by prompt engineering in 50%.\n"
      ],
      "metadata": {
        "id": "P3oM8-wnES1v"
      }
    }
  ]
}