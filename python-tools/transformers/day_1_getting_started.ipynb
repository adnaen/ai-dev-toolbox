{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOh7fzRNvJcV2WkTun9zh7l",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adnaen/ai-dev-toolbox/blob/main/python-tools/pypi_transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [`transformers`](https://huggingface.co/docs/transformers/en/index)\n",
        "\n",
        "*maintained by Hugging face*"
      ],
      "metadata": {
        "id": "G8dtpTO9PqF6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Installation**\n",
        "\n",
        "> ```bash\n",
        "> ~ pip install transformers\n",
        "> ```\n",
        "\n",
        "> ```bash\n",
        "> ~ pip install \"transformers[torch]\"   # for cpu\n",
        "> ```"
      ],
      "metadata": {
        "id": "bjFyz1LEP9Z-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### installation test"
      ],
      "metadata": {
        "id": "EVrt9ChIQZJB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers.pipelines import pipeline"
      ],
      "metadata": {
        "id": "BWb0aokiKTHF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loaded = pipeline('sentiment-analysis')"
      ],
      "metadata": {
        "id": "TkzqmPggKs-R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loaded(\"software development is fucking crazy!\")  # i'd love it, btw :)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oH4DeOIHNDW3",
        "outputId": "f89a30fd-a9a3-4395-dd82-00a2d7141338"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'label': 'NEGATIVE', 'score': 0.9961483478546143}]"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# in order to access private repo model from hugging face hub, we need to login\n",
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "NWG9n5q2c8xl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Base Classes**\n",
        "\n",
        "- **`PreTrainedConfig`** :  stores all **hyperparameter** and **architecture** *(like. voccab_size, no_of_attention_heads)*\n",
        "- **`PreTrainedModel`** : load model weight and resizing embeddings.\n",
        "- **`Preprocessor`** :  convert raw input to numerical model input *(raw text -> input tensor)*"
      ],
      "metadata": {
        "id": "TV5-oWPSIuUe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Generic Model Classes & Model Specific Classes**\n",
        "\n",
        "1. **Generic Model Class**\n",
        "    - This class can be used to load any model and it auto detect the models spefic config classes and load it.\n",
        "    - class name : `AutoModelForCausalLM`\n",
        "    - `Causal Model` : means, Decoder only model. It is used predict next word with given context. *(like. GPT, LLAMA, MISTRAL, PHI)*\n",
        "    - But, under the hood this detect the given model specifc class and load that.\n",
        "    - like.\n",
        "    ```python\n",
        "    model = AutoModelForCausalLM('openai-community/gpt2')\n",
        "    ```\n",
        "    it load the `GPT2Model` internally.\n",
        "\n",
        "2. **Model Specific Class**\n",
        "    - For load Gpt model we can use the GPT model class = `GPT2Model`\n",
        "    - Similary for `Llama, Mistral, Phi...`\n",
        "\n",
        "\n",
        "- **`AutoModelForCausalLM` is use where we dont know the exact model, or model will change on production**\n",
        "- **`ModelSpecificClass` use when we know the exact model.**"
      ],
      "metadata": {
        "id": "MMbrqAofP7Zg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> ### `?.` **Is all llm model can be loaded with `AutoModelForCausalLM`.**\n",
        "> ### `a:` No, `Causal Model` means, Decoder only model. it can only predict next word with given context. (like GPT, Mistral, Phi, Llama)\n",
        "So, **`AutoModelForCausalLM` can only load decoder only models.**"
      ],
      "metadata": {
        "id": "tzg6zASnTetp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Load a Decoder Model**"
      ],
      "metadata": {
        "id": "ijySKazrcPME"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO"
      ],
      "metadata": {
        "id": "rnUIpMgAcmqF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}